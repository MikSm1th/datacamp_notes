{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "mpl.rcParams['figure.figsize'] = [18,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric = ['BMI_class', 'Height_class', 'Gender', 'Component', 'Branch']\n",
    "regression = ['BMI_class', 'Height_class', 'Gender', 'Component', 'Branch', 'BMI']\n",
    "\n",
    "def load_ansur(cols_to_drop, test_size, model_type='class'):\n",
    "    df_m = pd.read_csv('data/ANSUR_II_MALE.csv')\n",
    "    df_f = pd.read_csv('data/ANSUR_II_FEMALE.csv')\n",
    "    ansur_df = pd.concat([df_m, df_f], axis=0)\n",
    "    \n",
    "    if model_type == 'class': \n",
    "        X = ansur_df.drop(non_numeric, axis=1)\n",
    "        y = ansur_df['Gender']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    elif model_type == 'reg':\n",
    "        X = ansur_df.drop(regression, axis=1)\n",
    "        y = ansur_df[\"BMI\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "    else:\n",
    "        print('please specify model type')\n",
    "        \n",
    "    return X, y, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_ansur(non_numeric, 0.3)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test_std)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the feature coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11968814  0.14414063 -0.08561985 -0.28054672 -0.05632391 -0.0621446\n",
      "  0.14700426  0.73677963  0.7013665  -0.83836831]\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abdominalextensiondepthsitting': 0.11968813731025733,\n",
       " 'acromialheight': 0.1441406275554733,\n",
       " 'acromionradialelength': 0.08561985044125861,\n",
       " 'anklecircumference': 0.2805467206936949,\n",
       " 'axillaheight': 0.05632390833085285,\n",
       " 'balloffootcircumference': 0.06214459531742643,\n",
       " 'balloffootlength': 0.14700426319715112,\n",
       " 'biacromialbreadth': 0.7367796309781828,\n",
       " 'bicepscircumferenceflexed': 0.7013665009994752,\n",
       " 'bicristalbreadth': 0.8383683123864404}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_dict = dict(zip(X.columns, abs(lr.coef_[0])))\n",
    "\n",
    "{k: v for i, (k, v) in enumerate(coef_dict.items()) if i < 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_coef = {k: v for k, v in coef_dict.items() if v < .401}\n",
    "\n",
    "cols = [k for k, v in low_coef.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "print(accuracy_score(y_test, lr.predict(scaler.transform(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_ansur(non_numeric, 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 l1_ratio=None, max_iter=100,\n",
       "                                 multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                                 random_state=None, solver='lbfgs', tol=0.0001,\n",
       "                                 verbose=0, warm_start=False),\n",
       "    n_features_to_select=5, step=1, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=5, verbose=0)\n",
    "rfe.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['biacromialbreadth', 'buttockcircumference', 'chestheight',\n",
       "       'lateralfemoralepicondyleheight', 'neckcircumference'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict(zip(X.columns, rfe.ranking_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9950576606260296\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, rfe.predict(X_test_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pima(cols_to_drop):\n",
    "    df = pd.read_csv('data/PimaIndians.csv')\n",
    "\n",
    "    X = df.drop(cols_to_drop, axis=1)\n",
    "    y = df['test']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    return X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "X, y, X_train, X_test, y_train, y_test = load_pima('test')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.4% accuracy on test set.\n",
      "{'pregnant': 0.21, 'glucose': 1.09, 'diastolic': 0.05, 'triceps': 0.11, 'insulin': 0.2, 'bmi': 0.54, 'family': 0.19, 'age': 0.51}\n"
     ]
    }
   ],
   "source": [
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scale the test features\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Predict diabetes presence on the scaled test set\n",
    "y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Prints accuracy metrics and feature coefficients\n",
    "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "{'pregnant': 1, 'glucose': 1, 'diastolic': 5, 'triceps': 2, 'insulin': 6, 'bmi': 4, 'family': 1, 'age': 3}\n",
      "Index(['pregnant', 'glucose', 'family'], dtype='object')\n",
      "78.8% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_pima('test')\n",
    "\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fits the eliminator to the data\n",
    "rfe.fit(X_train_std, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test_std))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_ansur(non_numeric, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9950576606260296\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00154633 0.00086357 0.00045535 0.00073979 0.00078243 0.00354988\n",
      " 0.0049853  0.11099988 0.00199698 0.00728055 0.01240843 0.03025133\n",
      " 0.00078188 0.00279489 0.00147059 0.01370766 0.00393419 0.00061608\n",
      " 0.00263703 0.00182259 0.00208289 0.00686943 0.00103596 0.00122278\n",
      " 0.00588273 0.03615508 0.00042108 0.00863875 0.00078667 0.00094452\n",
      " 0.00073667 0.00213626 0.00057075 0.00708263 0.00326338 0.02455007\n",
      " 0.00444625 0.03673032 0.00790558 0.00162086 0.00052534 0.03855737\n",
      " 0.04173666 0.00055813 0.00097183 0.00102382 0.00055843 0.0122277\n",
      " 0.00062119 0.0143125  0.02520539 0.00052106 0.00052685 0.00616485\n",
      " 0.0115588  0.00038734 0.00043615 0.00111944 0.01427075 0.00493086\n",
      " 0.00206494 0.1438532  0.10619575 0.00544109 0.00041621 0.00284787\n",
      " 0.00613423 0.05525179 0.00084518 0.00154307 0.00302382 0.03503822\n",
      " 0.00056371 0.00586678 0.00076146 0.00173843 0.01001096 0.00100699\n",
      " 0.0005667  0.00066962 0.00042994 0.00061887 0.00172817 0.0223065\n",
      " 0.00130516 0.00187261 0.0006996  0.00062953 0.00085361 0.02799959\n",
      " 0.00061364 0.00193941 0.01992729 0.00231421]\n"
     ]
    }
   ],
   "source": [
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False  True False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False  True False False False  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True False False False False  True False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "mask = rf.feature_importances_ > 0.03\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['biacromialbreadth', 'bimalleolarbreadth', 'chestheight',\n",
      "       'forearmcircumferenceflexed', 'handbreadth', 'handcircumference',\n",
      "       'neckcircumference', 'neckcircumferencebase', 'shouldercircumference',\n",
      "       'sleevelengthspinewrist'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_reduced = X_train.loc[:, mask]\n",
    "print(X_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE with random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 14 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                     class_weight=None, criterion='gini',\n",
       "                                     max_depth=None, max_features='auto',\n",
       "                                     max_leaf_nodes=None, max_samples=None,\n",
       "                                     min_impurity_decrease=0.0,\n",
       "                                     min_impurity_split=None,\n",
       "                                     min_samples_leaf=1, min_samples_split=2,\n",
       "                                     min_weight_fraction_leaf=0.0,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     oob_score=False, random_state=None,\n",
       "                                     verbose=0, warm_start=False),\n",
       "    n_features_to_select=6, step=10, verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(estimator=RandomForestClassifier(), \n",
    "          n_features_to_select=6, step=10, \n",
    "            verbose=1)\n",
    "\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['biacromialbreadth', 'handcircumference', 'neckcircumference',\n",
      "       'neckcircumferencebase', 'shouldercircumference', 'wristcircumference'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns[rfe.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_pima('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pregnant': 0.08, 'glucose': 0.26, 'diastolic': 0.06, 'triceps': 0.08, 'insulin': 0.16, 'bmi': 0.1, 'family': 0.1, 'age': 0.15}\n",
      "74.6% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "print(dict(zip(X_train.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False  True False False  True]\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "# Prints out the mask\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['glucose', 'insulin', 'age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X_train.loc[:, mask]\n",
    "\n",
    "# prints out the selected column names\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_pima('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 4 features.\n",
      "Index(['glucose', 'insulin'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Wrap the feature eliminator around the random forest model\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask using an attribute of rfe\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_ansur(non_numeric, 0.3, 'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model\n",
    "la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "la.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can predict 82.9% of the variance in the test set.\n",
      "The model has ignored 83 out of 93 features.\n"
     ]
    }
   ],
   "source": [
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can predict 82.9% of the variance in the test set.\n",
      "83 out of 93 features were ignored.\n",
      "The model can predict 90.8% of the variance in the test set.\n",
      "82 out of 93 features were ignored.\n",
      "The model can predict 98.5% of the variance in the test set.\n",
      "69 out of 93 features were ignored.\n",
      "The model can predict 99.3% of the variance in the test set.\n",
      "53 out of 93 features were ignored.\n"
     ]
    }
   ],
   "source": [
    "alphas = [1, 0.5, 0.1, 0.01]\n",
    "for a in alphas:\n",
    "    # Find the highest alpha value with R-squared above 98%\n",
    "    la = Lasso(alpha=a, random_state=0)\n",
    "\n",
    "    # Fits the model and calculates performance stats\n",
    "    la.fit(X_train_std, y_train)\n",
    "    r_squared = la.score(X_test_std, y_test)\n",
    "    n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "    # Print peformance stats \n",
    "    print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "    print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining feature selectors \n",
    "#### Taking a step back\n",
    "- Random forest is combination of decision trees \n",
    "- We can use combination of models for feature selection too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_ansur(non_numeric, 0.25, 'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9895676219048437"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "\n",
    "lcv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcv_mask = lcv.coef_ != 0\n",
    "sum(lcv_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 43 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                    criterion='mse', max_depth=None,\n",
       "                                    max_features='auto', max_leaf_nodes=None,\n",
       "                                    max_samples=None, min_impurity_decrease=0.0,\n",
       "                                    min_impurity_split=None, min_samples_leaf=1,\n",
       "                                    min_samples_split=2,\n",
       "                                    min_weight_fraction_leaf=0.0,\n",
       "                                    n_estimators=100, n_jobs=None,\n",
       "                                    oob_score=False, random_state=None,\n",
       "                                    verbose=0, warm_start=False),\n",
       "    n_features_to_select=38, step=5, verbose=5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(),\n",
    "            n_features_to_select=38, step=5, verbose=5)\n",
    "\n",
    "rfe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 43 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0,\n",
       "                                        criterion='friedman_mse', init=None,\n",
       "                                        learning_rate=0.1, loss='ls',\n",
       "                                        max_depth=3, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_iter_no_change=None,\n",
       "                                        presort='deprecated', random_state=None,\n",
       "                                        subsample=1.0, tol=0.0001,\n",
       "                                        validation_fraction=0.1, verbose=0,\n",
       "                                        warm_start=False),\n",
       "    n_features_to_select=38, step=5, verbose=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "            n_features_to_select=38, step=5, verbose=1)\n",
    "\n",
    "rfe_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mask = rfe_gb.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 0 1 3 0 0 0 3 0 3 0 2 0 1 3 3 1 0 1 3 3 0 3 3 1 1 1 1 0 0 0 1 2 1 0 0\n",
      " 3 3 0 1 0 1 0 0 0 0 1 1 0 0 3 1 2 0 0 0 1 0 3 0 3 0 1 0 2 0 3 0 0 3 1 0 1\n",
      " 2 2 3 3 1 3 0 2 1 1 0 3 3 1 3 0 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "votes =np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "print(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = votes >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_X = X.loc[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_ansur(non_numeric, 0.3, 'reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is different than the one in the datacamp exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha = 0.406\n",
      "The model explains 98.9% of the test set variance\n",
      "38 features out of 93 selected\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 13 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0,\n",
       "                                        criterion='friedman_mse', init=None,\n",
       "                                        learning_rate=0.1, loss='ls',\n",
       "                                        max_depth=3, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_iter_no_change=None,\n",
       "                                        presort='deprecated', random_state=None,\n",
       "                                        subsample=1.0, tol=0.0001,\n",
       "                                        validation_fraction=0.1, verbose=0,\n",
       "                                        warm_start=False),\n",
       "    n_features_to_select=10, step=20, verbose=1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 20 features on each step\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "             n_features_to_select=10, step=20, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 97.1% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 13 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                    criterion='mse', max_depth=None,\n",
       "                                    max_features='auto', max_leaf_nodes=None,\n",
       "                                    max_samples=None, min_impurity_decrease=0.0,\n",
       "                                    min_impurity_split=None, min_samples_leaf=1,\n",
       "                                    min_samples_split=2,\n",
       "                                    min_weight_fraction_leaf=0.0,\n",
       "                                    n_estimators=100, n_jobs=None,\n",
       "                                    oob_score=False, random_state=None,\n",
       "                                    verbose=0, warm_start=False),\n",
       "    n_features_to_select=10, step=20, verbose=1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 10 features with RFE on a RandomForestRegressor, drop 20 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "             n_features_to_select=10, step=20, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 97.5% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 0 0 3 0 0 0 3 0 1 0 1 0 0 1 3 0 0 0 3 1 0 3 2 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 1 1 0 1 0 0 0 0 0 0 1 0 0 0 2 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
      " 1 0 3 1 1 1 0 0 1 1 0 3 1 0 1 0 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, gb_mask, rf_mask], axis=0)\n",
    "print(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False  True False False False  True False False False\n",
      " False False False False  True False False False  True False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False  True False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "print(meta_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['axillaheight', 'bicepscircumferenceflexed', 'buttockdepth',\n",
      "       'calfcircumference', 'chestcircumference', 'thighcircumference',\n",
      "       'waistcircumference'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.iloc[:, meta_mask]\n",
    "print(X_reduced.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 97.1% of the variance in the test set using 7 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X, y, X_train, X_test, y_train, y_test = load_pima('test')\n",
    "X, y, X_train, X_test, y_train, y_test = load_ansur(non_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html 3_Screening_for_model_accuracy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../gitbsh > /dev/null 2>&1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
