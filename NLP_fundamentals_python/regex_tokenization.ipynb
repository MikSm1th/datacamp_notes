{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.match('abc', 'abcdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 2), match='hi'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_regex ='\\w+'\n",
    "re.match(word_regex, 'hi there!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>pattern</th>\n",
    "      <th>matches</th> \n",
    "      <th>example</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>\\w+</td>\n",
    "      <td>word</td> \n",
    "      <td>'Magic'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\\d</td>\n",
    "      <td>digit</td> \n",
    "      <td>9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\\s</td>\n",
    "      <td>space</td> \n",
    "      <td>' '</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>.*</td>\n",
    "      <td>wildcard</td> \n",
    "      <td>username74</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>+ or *</td>\n",
    "      <td>greedy match</td> \n",
    "      <td>'aaaaa'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\\S</td>\n",
    "      <td><b>not</b> space</td> \n",
    "      <td>'no_spaces'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>[a-z]</td>\n",
    "      <td>lowercase group</td> \n",
    "      <td>'abcdefg'</td>\n",
    "    </tr>        \n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python's re module\n",
    "- `re` module\n",
    "- `split:` split a string on regex\n",
    "- `findall:` find all patterns in a string\n",
    "- `search:` search for a pattern\n",
    "- `match:` match an entire string or substring based on a pattern \n",
    "<br>\n",
    "- Pass the pattern first, and then the string second\n",
    "- May return an iterator, string, or match object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'spaces.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'Split on spaces.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is tokenization?\n",
    "- Turning a string of document into **tokens** (smaller chunks)\n",
    "- One step in preparing a text for NLP\n",
    "- Many different theories and rules\n",
    "- Rules can be created using regular expressions \n",
    "- some examples:\n",
    " - Breaking out words or sentences\n",
    " - Seperating punctuation\n",
    " - Seperating all hashtags in a tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk library\n",
    "- `nltk:` natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('Hi there!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why tokenize?\n",
    "- Easier to map part of speech \n",
    "- Matching common words\n",
    "- Removing unwanted tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other nltk tokenizers\n",
    "- `sent_tokenize:` tokenize a document into sentences\n",
    "- `regexp_tokenize:` tokenize a string or document based on a regular expression pattern \n",
    "- `TweetTokenizer:` special calls just for tweet tokenization, allowing you to seperate hashtags, mentions and lots of exclamation points!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex practice\n",
    "- Difference between `re.search()` and `re.match()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('grail.txt', 'r')\n",
    "content = f.read()\n",
    "SCENES = content.split('SCENE')\n",
    "scene_one = 'SCENE' + SCENES[1]\n",
    "#scene_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an', 'in', 'SCENE', 'are', 'right', 'me', 'weight', 'defeator', 'velocity', \"'ve\", 'bangin', 'Yes', 'then', 'needs', 'King', 'minute', 'No', 'feathers', 'get', ':', 'ounce', 'Please', \"'\", 'beat', 'guiding', '...', 'Wait', 'together', 'on', 'goes', 'air-speed', 'them', 'using', 'trusty', 'servant', 'carried', 'wind', 'to', 'non-migratory', 'castle', 'African', 'they', 'question', 'by', 'interested', 'ask', 'That', 'you', '--', 'the', 'grip', 'these', 'five', 'one', 'SOLDIER', 'there', 'he', \"'s\", 'and', 'master', '1', 'plover', 'Not', 'with', 'of', 'ridden', 'Patsy', 'every', 'halves', 'Court', 'But', 'length', \"'em\", 'son', 'England', 'So', 'covered', ']', 'Found', 'horse', 'We', 'simple', 'held', 'strangers', 'maybe', ',', 'since', 'zone', 'carry', 'ratios', 'Are', 'tell', 'carrying', 'order', 'court', 'two', 'Britons', 'other', 'land', 'will', 'kingdom', 'Mercea', 'climes', '!', 'warmer', 'line', 'KING', 'a', 'Who', 'snows', 'ARTHUR', 'under', 'speak', 'fly', 'Where', 'do', 'south', 'agree', 'I', 'here', 'A', 'Listen', 'matter', 'Camelot', \"'re\", 'suggesting', 'if', 'Well', 'migrate', 'forty-three', '#', 'sun', 'must', \"'m\", 'It', 'The', 'second', 'Saxons', 'back', 'your', 'but', 'house', 'why', 'have', 'our', 'Pendragon', 'breadth', 'join', 'creeper', 'Will', 'yeah', 'my', 'point', '.', 'anyway', 'course', 'not', 'seek', 'swallows', 'be', 'all', 'that', 'this', 'winter', 'may', 'from', '[', 'empty', 'found', 'Arthur', 'European', \"'d\", 'pound', 'times', 'just', 'swallow', 'Ridden', 'grips', 'clop', 'search', 'What', 'go', 'Supposing', 'does', 'sovereign', 'strand', 'got', 'is', 'Pull', \"n't\", 'bird', 'use', 'through', '?', 'lord', 'dorsal', 'Uther', 'In', 'could', 'where', 'coconut', 'maintain', 'martin', 'mean', 'wings', 'You', 'yet', 'am', 'who', 'husk', 'tropical', 'bring', 'Am', 'knights', 'temperate', 'its', 'or', 'coconuts', 'They', 'Halt', 'Oh', 'wants', 'it', '2', 'at', 'Whoa'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<_sre.SRE_Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[ARTHUR:]+\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advancing tokenization with regex\n",
    "#### Regex groups using or \"|\"\n",
    "- OR is represented using |\n",
    "- Define a group using ()\n",
    "- Define explicit character ranges using []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "re.findall(match_digits_and_words, 'He has 11 cats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>pattern</th>\n",
    "      <th>matches</th> \n",
    "      <th>example</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>[A-Za-z]+</td>\n",
    "      <td>upper and lowercase English Alphabet</td> \n",
    "      <td>'ABCDEFghijk'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>[0-9]</td>\n",
    "      <td>number from 0 to 9</td> \n",
    "      <td>9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>[A-Za-z\\-\\.]+</td>\n",
    "      <td>upper and lower case English alphabet, - and .</td> \n",
    "      <td>'My-Website.com'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>(a-z)</td>\n",
    "      <td>a, -, and z</td> \n",
    "      <td>username74</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>+ or *</td>\n",
    "      <td>greedy match</td> \n",
    "      <td>'a-z'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>(\\s+|,)</td>\n",
    "      <td>spaces or a comma</td> \n",
    "      <td>', '</td>\n",
    "    </tr>        \n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text='Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[ÃœA-Z]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting word length with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 0., 1., 0., 0., 0., 3., 0., 0., 1.]),\n",
       " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = word_tokenize('This is a pretty cool tool!')\n",
    "word_lengths = [len(w) for w in words]\n",
    "plt.hist(word_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('grail.txt', 'r')\n",
    "holy_grail = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, '\\w+') for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
