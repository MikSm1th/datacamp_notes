{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.match('abc', 'abcdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 2), match='hi'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_regex ='\\w+'\n",
    "re.match(word_regex, 'hi there!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>pattern</th>\n",
    "      <th>matches</th> \n",
    "      <th>example</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>\\w+</td>\n",
    "      <td>word</td> \n",
    "      <td>'Magic'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\\d</td>\n",
    "      <td>digit</td> \n",
    "      <td>9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\\s</td>\n",
    "      <td>space</td> \n",
    "      <td>' '</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>.*</td>\n",
    "      <td>wildcard</td> \n",
    "      <td>username74</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>+ or *</td>\n",
    "      <td>greedy match</td> \n",
    "      <td>'aaaaa'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>\\S</td>\n",
    "      <td><b>not</b> space</td> \n",
    "      <td>'no_spaces'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>[a-z]</td>\n",
    "      <td>lowercase group</td> \n",
    "      <td>'abcdefg'</td>\n",
    "    </tr>        \n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python's re module\n",
    "- `re` module\n",
    "- `split:` split a string on regex\n",
    "- `findall:` find all patterns in a string\n",
    "- `search:` search for a pattern\n",
    "- `match:` match an entire string or substring based on a pattern \n",
    "<br>\n",
    "- Pass the pattern first, and then the string second\n",
    "- May return an iterator, string, or match object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'spaces.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'Split on spaces.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is tokenization?\n",
    "- Turning a string of document into **tokens** (smaller chunks)\n",
    "- One step in preparing a text for NLP\n",
    "- Many different theories and rules\n",
    "- Rules can be created using regular expressions \n",
    "- some examples:\n",
    " - Breaking out words or sentences\n",
    " - Seperating punctuation\n",
    " - Seperating all hashtags in a tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk library\n",
    "- `nltk:` natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('Hi there!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why tokenize?\n",
    "- Easier to map part of speech \n",
    "- Matching common words\n",
    "- Removing unwanted tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other nltk tokenizers\n",
    "- `sent_tokenize:` tokenize a document into sentences\n",
    "- `regexp_tokenize:` tokenize a string or document based on a regular expression pattern \n",
    "- `TweetTokenizer:` special calls just for tweet tokenization, allowing you to seperate hashtags, mentions and lots of exclamation points!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex practice\n",
    "- Difference between `re.search()` and `re.match()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('grail.txt', 'r')\n",
    "content = f.read()\n",
    "SCENES = content.split('SCENE')\n",
    "scene_one = 'SCENE' + SCENES[1]\n",
    "#scene_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Uther', 'this', '1', 'Court', 'Found', 'search', 'halves', \"'\", 'Yes', 'at', 'point', '!', 'are', 'or', 'horse', 'by', 'That', 'then', 'court', '2', 'a', 'coconut', 'Well', 'Wait', 'an', 'agree', 'together', 'weight', 'they', 'Not', 'simple', 'ounce', 'Mercea', 'snows', 'Pull', 'from', 'Camelot', 'all', 'two', \"'ve\", 'its', \"'s\", 'ARTHUR', 'Am', 'anyway', 'sovereign', 'trusty', 'on', 'seek', 'these', 'right', 'there', 'knights', ']', 'the', 'grip', \"'m\", 'But', 'line', 'lord', 'needs', 'south', \"'re\", 'migrate', 'Please', 'question', 'kingdom', 'grips', 'Where', 'my', 'using', 'King', 'SCENE', 'Listen', 'house', 'velocity', 'dorsal', 'swallow', 'defeator', 'non-migratory', 'matter', '#', 'bird', 'warmer', 'them', 'may', 'held', 'winter', 'that', 'European', 'Are', 'speak', 'get', 'strangers', 'plover', 'carry', 'do', 'forty-three', 'interested', 'five', 'Pendragon', 'martin', 'be', 'join', 'No', 'who', ',', 'one', 'not', '.', 'servant', \"n't\", 'maintain', 'wants', 'pound', 'other', 'here', 'Britons', 'breadth', 'strand', 'Will', 'our', 'ask', 'SOLDIER', 'me', 'every', 'does', 'why', 'tropical', 'yeah', 'tell', 'could', 'empty', 'Ridden', 'guiding', 'swallows', 'Saxons', 'beat', 'order', 'you', 'must', 'Oh', 'We', 'coconuts', 'zone', 'castle', 'carrying', 'You', 'have', 'in', 'temperate', 'goes', 'mean', 'length', 'with', 'ratios', 'creeper', 'wings', 'found', 'maybe', 'wind', 'under', '...', 'feathers', \"'em\", 'sun', 'minute', 'back', 'Who', '?', 'Halt', 'and', 'second', 'he', 'A', 'clop', \"'d\", 'is', 'through', 'So', 'It', 'England', 'since', 'What', 'course', 'covered', ':', 'just', 'am', 'suggesting', 'Arthur', 'Patsy', 'In', 'husk', 'They', 'climes', 'if', 'African', 'will', 'fly', 'but', 'use', 'got', 'it', 'go', 'The', 'where', 'to', 'times', 'bring', 'yet', 'I', 'air-speed', 'ridden', 'son', 'your', '--', 'Supposing', '[', 'land', 'carried', 'KING', 'bangin', 'Whoa', 'of', 'master'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<_sre.SRE_Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[ARTHUR:]+\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advancing tokenization with regex\n",
    "#### Regex groups using or \"|\"\n",
    "- OR is represented using |\n",
    "- Define a group using ()\n",
    "- Define explicit character ranges using []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "re.findall(match_digits_and_words, 'He has 11 cats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>pattern</th>\n",
    "      <th>matches</th> \n",
    "      <th>example</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>[A-Za-z]+</td>\n",
    "      <td>upper and lowercase English Alphabet</td> \n",
    "      <td>'ABCDEFghijk'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>[0-9]</td>\n",
    "      <td>number from 0 to 9</td> \n",
    "      <td>9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>[A-Za-z\\-\\.]+</td>\n",
    "      <td>upper and lower case English alphabet, - and .</td> \n",
    "      <td>'My-Website.com'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>(a-z)</td>\n",
    "      <td>a, -, and z</td> \n",
    "      <td>username74</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>+ or *</td>\n",
    "      <td>greedy match</td> \n",
    "      <td>'a-z'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>(\\s+|,)</td>\n",
    "      <td>spaces or a comma</td> \n",
    "      <td>', '</td>\n",
    "    </tr>        \n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 35), match='match lowercase spaces nums like 12'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text='Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[ÃœA-Z]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting word length with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 0., 1., 0., 0., 0., 3., 0., 0., 1.]),\n",
       " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = word_tokenize('This is a pretty cool tool!')\n",
    "word_lengths = [len(w) for w in words]\n",
    "plt.hist(word_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
