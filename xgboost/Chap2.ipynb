{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "mpl.rcParams['figure.figsize'] = [12,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression review\n",
    "- Outcome is real valued\n",
    "\n",
    "### Common regression metrics\n",
    "- Root mean squared error (RMSE)\n",
    "- Mean absolute error (MAE)\n",
    "\n",
    "### Common regression algorithms\n",
    "- Linear regression\n",
    "- Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective (loss) functions and base learners\n",
    "- Quantifies how far off a prediction is from the actual result\n",
    "- Measures the difference between estimated and true values for some collection of data\n",
    "- Goal: Find the model that yields the minimum value of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
       "       'YearBuilt', 'Remodeled', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\n",
       "       'FullBath', 'HalfBath', 'BedroomAbvGr', 'Fireplaces', 'GarageArea',\n",
       "       'MSZoning_FV', 'MSZoning_RH', 'MSZoning_RL', 'MSZoning_RM',\n",
       "       'Neighborhood_Blueste', 'Neighborhood_BrDale', 'Neighborhood_BrkSide',\n",
       "       'Neighborhood_ClearCr', 'Neighborhood_CollgCr', 'Neighborhood_Crawfor',\n",
       "       'Neighborhood_Edwards', 'Neighborhood_Gilbert', 'Neighborhood_IDOTRR',\n",
       "       'Neighborhood_MeadowV', 'Neighborhood_Mitchel', 'Neighborhood_NAmes',\n",
       "       'Neighborhood_NPkVill', 'Neighborhood_NWAmes', 'Neighborhood_NoRidge',\n",
       "       'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_SWISU',\n",
       "       'Neighborhood_Sawyer', 'Neighborhood_SawyerW', 'Neighborhood_Somerst',\n",
       "       'Neighborhood_StoneBr', 'Neighborhood_Timber', 'Neighborhood_Veenker',\n",
       "       'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs',\n",
       "       'BldgType_TwnhsE', 'HouseStyle_1.5Unf', 'HouseStyle_1Story',\n",
       "       'HouseStyle_2.5Fin', 'HouseStyle_2.5Unf', 'HouseStyle_2Story',\n",
       "       'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'PavedDrive_P', 'PavedDrive_Y',\n",
       "       'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/ames_housing_trimmed_processed.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['SalePrice'], axis=1)\n",
    "y = df.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reg:linear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbooster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gbtree'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_delta_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolsample_bylevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolsample_bynode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreg_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreg_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbase_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mimportance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gain'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Implementation of the scikit-learn API for XGBoost regression.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "max_depth : int\n",
       "    Maximum tree depth for base learners.\n",
       "learning_rate : float\n",
       "    Boosting learning rate (xgb's \"eta\")\n",
       "n_estimators : int\n",
       "    Number of trees to fit.\n",
       "verbosity : int\n",
       "    The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
       "silent : boolean\n",
       "    Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
       "objective : string or callable\n",
       "    Specify the learning task and the corresponding learning objective or\n",
       "    a custom objective function to be used (see note below).\n",
       "booster: string\n",
       "    Specify which booster to use: gbtree, gblinear or dart.\n",
       "nthread : int\n",
       "    Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
       "n_jobs : int\n",
       "    Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
       "gamma : float\n",
       "    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
       "min_child_weight : int\n",
       "    Minimum sum of instance weight(hessian) needed in a child.\n",
       "max_delta_step : int\n",
       "    Maximum delta step we allow each tree's weight estimation to be.\n",
       "subsample : float\n",
       "    Subsample ratio of the training instance.\n",
       "colsample_bytree : float\n",
       "    Subsample ratio of columns when constructing each tree.\n",
       "colsample_bylevel : float\n",
       "    Subsample ratio of columns for each level.\n",
       "colsample_bynode : float\n",
       "    Subsample ratio of columns for each split.\n",
       "reg_alpha : float (xgb's alpha)\n",
       "    L1 regularization term on weights\n",
       "reg_lambda : float (xgb's lambda)\n",
       "    L2 regularization term on weights\n",
       "scale_pos_weight : float\n",
       "    Balancing of positive and negative weights.\n",
       "base_score:\n",
       "    The initial prediction score of all instances, global bias.\n",
       "seed : int\n",
       "    Random number seed.  (Deprecated, please use random_state)\n",
       "random_state : int\n",
       "    Random number seed.  (replaces seed)\n",
       "missing : float, optional\n",
       "    Value in the data which needs to be present as a missing value. If\n",
       "    None, defaults to np.nan.\n",
       "importance_type: string, default \"gain\"\n",
       "    The feature importance type for the feature_importances_ property: either \"gain\",\n",
       "    \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
       "\\*\\*kwargs : dict, optional\n",
       "    Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
       "    be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
       "    Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
       "    will result in a TypeError.\n",
       "\n",
       "    .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
       "\n",
       "        \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
       "        passed via this argument will interact properly with scikit-learn.\n",
       "\n",
       "Note\n",
       "----\n",
       "A custom objective function can be provided for the ``objective``\n",
       "parameter. In this case, it should have the signature\n",
       "``objective(y_true, y_pred) -> grad, hess``:\n",
       "\n",
       "y_true: array_like of shape [n_samples]\n",
       "    The target values\n",
       "y_pred: array_like of shape [n_samples]\n",
       "    The predicted values\n",
       "\n",
       "grad: array_like of shape [n_samples]\n",
       "    The value of the gradient for each sample point.\n",
       "hess: array_like of shape [n_samples]\n",
       "    The value of the second derivative for each sample point\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     XGBRFRegressor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?xgb.XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:33:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE: 78847.401758\n"
     ]
    }
   ],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(preds, y_test))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlearning_rates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Train a booster with given parameters.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "params : dict\n",
       "    Booster params.\n",
       "dtrain : DMatrix\n",
       "    Data to be trained.\n",
       "num_boost_round: int\n",
       "    Number of boosting iterations.\n",
       "evals: list of pairs (DMatrix, string)\n",
       "    List of items to be evaluated during training, this allows user to watch\n",
       "    performance on the validation set.\n",
       "obj : function\n",
       "    Customized objective function.\n",
       "feval : function\n",
       "    Customized evaluation function.\n",
       "maximize : bool\n",
       "    Whether to maximize feval.\n",
       "early_stopping_rounds: int\n",
       "    Activates early stopping. Validation error needs to decrease at least\n",
       "    every **early_stopping_rounds** round(s) to continue training.\n",
       "    Requires at least one item in **evals**.\n",
       "    If there's more than one, will use the last.\n",
       "    Returns the model from the last iteration (not the best one).\n",
       "    If early stopping occurs, the model will have three additional fields:\n",
       "    ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.\n",
       "    (Use ``bst.best_ntree_limit`` to get the correct value if\n",
       "    ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)\n",
       "evals_result: dict\n",
       "    This dictionary stores the evaluation results of all the items in watchlist.\n",
       "\n",
       "    Example: with a watchlist containing\n",
       "    ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
       "    a parameter containing ``('eval_metric': 'logloss')``,\n",
       "    the **evals_result** returns\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        {'train': {'logloss': ['0.48253', '0.35953']},\n",
       "         'eval': {'logloss': ['0.480385', '0.357756']}}\n",
       "\n",
       "verbose_eval : bool or int\n",
       "    Requires at least one item in **evals**.\n",
       "    If **verbose_eval** is True then the evaluation metric on the validation set is\n",
       "    printed at each boosting stage.\n",
       "    If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
       "    is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
       "    / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
       "    Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
       "    is printed every 4 boosting stages, instead of every boosting stage.\n",
       "learning_rates: list or function (deprecated - use callback API instead)\n",
       "    List of learning rate for each boosting round\n",
       "    or a customized function that calculates eta in terms of\n",
       "    current number of round and the total number of boosting round (e.g. yields\n",
       "    learning rate decay)\n",
       "xgb_model : file name of stored xgb model or 'Booster' instance\n",
       "    Xgb model to be loaded before training (allows training continuation).\n",
       "callbacks : list of callback functions\n",
       "    List of callback functions that are applied at end of each iteration.\n",
       "    It is possible to use predefined callbacks by using\n",
       "    :ref:`Callback API <callback_api>`.\n",
       "    Example:\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        [xgb.callback.reset_learning_rate(custom_rates)]\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Booster : a trained booster model\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?xgb.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE: 44331.645061\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:51:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:51:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:51:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:51:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0    141767.488281      429.449371   142980.464844    1193.806011\n",
      "1    102832.562500      322.503447   104891.398438    1223.161012\n",
      "2     75872.621094      266.493573    79478.947265    1601.341377\n",
      "3     57245.657226      273.633063    62411.919922    2220.151162\n",
      "4     44401.291992      316.426590    51348.276367    2963.378029\n",
      "4    51348.276367\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params,\n",
    "                    nfold=4, num_boost_round=5, \n",
    "                    metrics='rmse', as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:53:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:53:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:53:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:53:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
      "0   127343.595703     668.167771  127634.185547   2404.009753\n",
      "1    89770.031250     456.980559   90122.505860   2107.916842\n",
      "2    63580.782226     263.442189   64278.558594   1887.552548\n",
      "3    45633.181640     151.849960   46819.175781   1459.821980\n",
      "4    33587.097656      87.003217   35670.655274   1140.613227\n",
      "4    35670.655274\n",
      "Name: test-mae-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params,\n",
    "                    nfold=4, num_boost_round=5, \n",
    "                    metrics='mae', as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and base learners in XGBoost\n",
    "- Regularization is a control on model complexity\n",
    "- Want models that are both accurate and as simple as possible\n",
    "- Regularization parameters in XGBoost:\n",
    "    - gamma - minimum loss reduction allowed for a split to occur\n",
    "    - alpha - l1 regularization on leaf weights, larger values mean more regularization \n",
    "    - lambda - l2 regularization on leaf weights\n",
    "    \n",
    "```python\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "boston_data = pd.read_csv('boston_data.csv')\n",
    "X,y = boston_data.iloc[:,:-1], boston_data.iloc[:, -1]\n",
    "\n",
    "boston_dmatrix = xbg.DMatrix(data=X, label=y)\n",
    "params = {'objective':'reg:linear', 'max_depth':4}\n",
    "\n",
    "l1_params = [1, 10, 100]\n",
    "rmses_l1 = []\n",
    "\n",
    "for reg in l1_params:\n",
    "    params['alpha'] = reg\n",
    "    cv_results = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=4,\n",
    "                        num_boost_round=10, metrics='rmse', \n",
    "                        as_pandas=True, seed=123)\n",
    "    rmses_l1.append(cv_results['test-rmse-mean'].tail(1).values[0])\n",
    "print('Best rmse as a function of l1: ')\n",
    "print(pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=['l1', 'rmse']))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base learners in XGBoost\n",
    "- Linear Base Learner:\n",
    "    - Sum of linear terms\n",
    "    - Boosted model is weighted sum of linear models\n",
    "        - Thus is itself linear\n",
    "    - Rarely used, can get the same performance from a regularized model\n",
    "- Tree Base Learner:\n",
    "    - Decision tree\n",
    "    - Boosted model is weighted sum of decision trees (non linear)\n",
    "    - Almost exclusively used in XGBoost\n",
    "\n",
    "### Creating DataFrames for multiple equal-length\n",
    "`pd.DataFrame(list(zip(list1, list2)), columns=['list1','list2']))`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
