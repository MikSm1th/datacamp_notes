{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "mpl.rcParams['figure.figsize'] = [24,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why tune your model? \n",
    "\n",
    "#### Untuned model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "housing_data = pd.read_csv('data/ames_housing_trimmed_processed.csv')\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuned rmse: 34624.229980\n"
     ]
    }
   ],
   "source": [
    "untuned_params={'objective':'reg:squarederror'}\n",
    "untuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix,\n",
    "                                params=untuned_params,\n",
    "                                nfold=4, metrics=\"rmse\",\n",
    "                                as_pandas=True, seed=123)\n",
    "print('Untuned rmse: %f' %((untuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned rmse: 31111.041992\n"
     ]
    }
   ],
   "source": [
    "tuned_params = {\"objective\":\"reg:squarederror\", \"colsample_bytree\": 0.3,\n",
    "               \"learning_rate\":0.1, \"max_depth\":5}\n",
    "\n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, \n",
    "                              params=tuned_params, nfold=4, \n",
    "                              num_boost_round=200, metrics=\"rmse\",\n",
    "                              as_pandas=True, seed=123)\n",
    "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse['test-rmse-mean']).tail(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.300781\n",
      "1                   10  34774.192708\n",
      "2                   15  32895.098307\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_matrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {'objective':'reg:squarederror', 'max_depth':3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse pr XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, \n",
    "                       nfold=3, num_boost_round=curr_num_rounds,\n",
    "                       metrics='rmse', as_pandas=True, \n",
    "                       seed=123)\n",
    "    \n",
    "    final_rmse_per_round.append(cv_results['test-rmse-mean'].tail().values[-1])\n",
    "    \n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses, columns=['num_boosting_rounds','rmse']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "47     11071.315430      604.089695    30732.664062    1966.998275\n",
      "48     10950.778646      574.862348    30712.240885    1957.751118\n",
      "49     10824.865560      576.666458    30720.854818    1950.511520\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params,\n",
    "                   metrics='rmse', nfold=3, \n",
    "                   early_stopping_rounds=10,\n",
    "                   num_boost_round=50, as_pandas=True,\n",
    "                   seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results.iloc[-3:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common tree tunable parameters\n",
    "- **learning rate:** learning rate/eta\n",
    "- **gamma:** min loss reduction to create new tree split\n",
    "- **lambda:** L2 (lasso) reg on leaf weights\n",
    "- **alpha:** L1 (ridge) ref on leaf weights\n",
    "- **max_depths:** max depth per tree\n",
    "- **subsample:** % samples used per tree\n",
    "- **colsample_bytree** % features used per tree \n",
    "\n",
    "### Linear tunable parameters\n",
    "- **lambda:** L2 reg on weights\n",
    "- **alpha:** L1 reg on weights\n",
    "- **lambda_bias:** L2 reg term on bias\n",
    "- You can also tune the number of estimators used for both base model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.401042\n",
      "1  0.010  179932.177083\n",
      "2  0.100   79759.414063\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Symetrically vary the eta\n",
    "for curr_val in eta_vals:\n",
    "    \n",
    "    params['eta'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results \n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params,\n",
    "                       metrics='rmse', nfold=3,\n",
    "                       early_stopping_rounds=5, \n",
    "                       num_boost_round=10, as_pandas=True,\n",
    "                       seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results['test-rmse-mean'].tail().values[-1])\n",
    "    \n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=['eta','best_rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37044.029948\n",
      "1          5  33210.039063\n",
      "2         10  34503.430990\n",
      "3         20  34847.684896\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:squarederror\"}\n",
    "\n",
    "# Create a list of max_depth values\n",
    "max_depths = [2,5,10,20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the maxabs\n",
    "for curr_value in max_depths:\n",
    "    \n",
    "    params['max_depth'] = curr_value\n",
    "    \n",
    "    # Perform cross_validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params,\n",
    "                       metrics='rmse', nfold=3,\n",
    "                       early_stopping_rounds=5,\n",
    "                       num_boost_round=10, \n",
    "                       as_pandas=True,\n",
    "                       seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results['test-rmse-mean'].tail().values[-1])\n",
    "    \n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)), columns=['max_depth', 'best_rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  48193.453125\n",
      "1               0.5  36013.541015\n",
      "2               0.8  35932.962891\n",
      "3               1.0  35836.042969\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:squarederror\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params['colsample_bytree'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of grid search and random search\n",
    "\n",
    "#### Grid search: review\n",
    "- Search exhaustively over a given set of hyperparameters, once per set of hyperparameters\n",
    "- Number of models = number of distinct values per hyperparameter multiplied across each hyperparameter\n",
    "- Pick final model hyperparameter values that give best cross-validated evaluation metric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   27.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5}\n",
      "Lowest RMSE found:  28410.039476552454\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "housing_data = pd.read_csv('data/ames_housing_trimmed_processed.csv')\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]],\\\n",
    "       housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "gbm_param_grid = {'learning_rate': [0.01, 0.1, 0.5, 0.9],\n",
    "                  'n_estimators': [200],\n",
    "                  'subsample': [0.3,0.5,0.9]}\n",
    "\n",
    "gbm = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "grid_mse.fit(X,y)\n",
    "\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \",np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search: review\n",
    "- Create a (possibly infinite) range of hyperparameter values per hyperparameter that you would like to search over\n",
    "- Set the number of iterations you would like for the random search to continue \n",
    "- During each iteration, randomly draw a value in the range of specified values for each hyperparameter searched over and train/evaluate a model with those hyperparameters\n",
    "- After you've reached the maximum number of iterations, select the hyperparameter configuration with the best evaluated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   57.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 0.35000000000000003, 'n_estimators': 200, 'learning_rate': 0.15000000000000002}\n",
      "Lowest RMSE found:  27959.816747233395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbm_param_grid = {'learning_rate': np.arange(0.05,1.05,0.05),\n",
    "                 'n_estimators': [200],\n",
    "                 'subsample': np.arange(0.05,1.05,0.05)}\n",
    "\n",
    "gbm = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,\n",
    "                                   n_iter=25, scoring='neg_mean_squared_error',\n",
    "                                   cv=4, verbose=1)\n",
    "\n",
    "randomized_mse.fit(X,y)\n",
    "\n",
    "print('Best parameters found: ', randomized_mse.best_params_)\n",
    "print('Lowest RMSE found: ', np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  29916.562522854438\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "                             scoring='neg_mean_squared_error', cv=4,\n",
    "                             verbose=1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
